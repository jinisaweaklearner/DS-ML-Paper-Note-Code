
- Batch Gradient Descent. Batch Size = Size of Training Set
- Stochastic Gradient Descent. Batch Size = 1
- Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set

It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize

Advantages and disadvantages
- https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network?newreg=8b8d00705bd046ba9be3a2fade087395