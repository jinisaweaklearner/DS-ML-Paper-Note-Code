### Definition
The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.
- https://www.tensorflow.org/text/guide/word_embeddings

Also, we can treate embedding as a fully connected layers
- https://towardsdatascience.com/convolutional-layers-vs-fully-connected-layers-364f05ab460b

### explain embedding layer
- https://zhuanlan.zhihu.com/p/83814532
- https://spaces.ac.cn/archives/4122

### How does Embedding Layer work?
- https://github.com/keras-team/keras/issues/3110#issuecomment-345153450


Skip-gram Embeddings example
- https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296